{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTc65T4c1hOI"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu rouge-score nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources for METEOR\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "KdAcuco61zvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "\n",
        "# 1) Read the dataset and create a working copy\n",
        "df_original = pd.read_csv(\"test_with_both_titles2.csv\")\n",
        "df_scores = df_original.copy()\n",
        "if \"abstract\" in df_scores.columns:\n",
        "    df_scores.drop(columns=[\"abstract\"], inplace=True)\n",
        "del df_original\n",
        "\n",
        "# 2) Extract reference and hypothesis lists\n",
        "refs = df_scores[\"title\"].tolist()\n",
        "hyps = df_scores[\"generated_title_finetuned\"].tolist()\n",
        "\n",
        "# 3) Compute BLEU-1 and BLEU-2 scores\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu1, bleu2 = [], []\n",
        "\n",
        "print(\"Calculating BLEU scores...\")\n",
        "for ref, hyp in tqdm(zip(refs, hyps), total=len(refs)):\n",
        "    ref_tok = ref.split()\n",
        "    hyp_tok = hyp.split()\n",
        "\n",
        "    score1 = sentence_bleu([ref_tok], hyp_tok, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
        "    bleu1.append(score1 * 100)\n",
        "\n",
        "    score2 = sentence_bleu([ref_tok], hyp_tok, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
        "    bleu2.append(score2 * 100)\n",
        "\n",
        "# 4) Compute ROUGE-1, ROUGE-2, and ROUGE-L\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge1, rouge2, rougeL = [], [], []\n",
        "\n",
        "print(\"Calculating ROUGE scores...\")\n",
        "for ref, hyp in tqdm(zip(refs, hyps), total=len(refs)):\n",
        "    scores = scorer.score(ref, hyp)\n",
        "    rouge1.append(scores[\"rouge1\"].fmeasure * 100)\n",
        "    rouge2.append(scores[\"rouge2\"].fmeasure * 100)\n",
        "    rougeL.append(scores[\"rougeL\"].fmeasure * 100)\n",
        "\n",
        "# 5) Compute METEOR scores\n",
        "print(\"Calculating METEOR scores...\")\n",
        "meteor = [\n",
        "    single_meteor_score(ref.split(), hyp.split()) * 100\n",
        "    for ref, hyp in tqdm(zip(refs, hyps), total=len(refs))\n",
        "]\n",
        "\n",
        "# 6) Add scores to the DataFrame\n",
        "df_scores[\"BLEU-1\"]  = bleu1\n",
        "df_scores[\"BLEU-2\"]  = bleu2\n",
        "df_scores[\"ROUGE-1\"] = rouge1\n",
        "df_scores[\"ROUGE-2\"] = rouge2\n",
        "df_scores[\"ROUGE-L\"] = rougeL\n",
        "df_scores[\"METEOR\"]  = meteor\n",
        "\n",
        "# 7) Save the DataFrame to a new CSV file\n",
        "df_scores = df_scores.round(3)\n",
        "\n",
        "\n",
        "# 8) Compute and print average scores\n",
        "metrics = [\"BLEU-1\", \"BLEU-2\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"METEOR\"]\n",
        "means = df_scores[metrics].mean()\n",
        "\n",
        "print(\"\\n--- Average Metric Scores ---\")\n",
        "for metric, value in means.items():\n",
        "    print(f\"{metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "id": "v7Zw1rha11SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.head()"
      ],
      "metadata": {
        "id": "mjtIK9EF5Wlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "\n",
        "# 1) Read the dataset and create a working copy\n",
        "df_original = pd.read_csv(\"test_with_both_titles2.csv\")\n",
        "df_scores_base = df_original.copy()\n",
        "if \"abstract\" in df_scores_base.columns:\n",
        "    df_scores_base.drop(columns=[\"abstract\"], inplace=True)\n",
        "del df_original\n",
        "\n",
        "# 2) Extract reference and hypothesis lists\n",
        "refs = df_scores_base[\"title\"].tolist()\n",
        "hyps = df_scores_base[\"generated_title_base\"].tolist()\n",
        "\n",
        "# 3) Compute BLEU-1 and BLEU-2 scores\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu1, bleu2 = [], []\n",
        "\n",
        "print(\"Calculating BLEU scores...\")\n",
        "for ref, hyp in tqdm(zip(refs, hyps), total=len(refs)):\n",
        "    ref_tok = ref.split()\n",
        "    hyp_tok = hyp.split()\n",
        "\n",
        "    score1 = sentence_bleu([ref_tok], hyp_tok, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
        "    bleu1.append(score1 * 100)\n",
        "\n",
        "    score2 = sentence_bleu([ref_tok], hyp_tok, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
        "    bleu2.append(score2 * 100)\n",
        "\n",
        "# 4) Compute ROUGE-1, ROUGE-2, and ROUGE-L\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge1, rouge2, rougeL = [], [], []\n",
        "\n",
        "print(\"Calculating ROUGE scores...\")\n",
        "for ref, hyp in tqdm(zip(refs, hyps), total=len(refs)):\n",
        "    scores = scorer.score(ref, hyp)\n",
        "    rouge1.append(scores[\"rouge1\"].fmeasure * 100)\n",
        "    rouge2.append(scores[\"rouge2\"].fmeasure * 100)\n",
        "    rougeL.append(scores[\"rougeL\"].fmeasure * 100)\n",
        "\n",
        "# 5) Compute METEOR scores\n",
        "print(\"Calculating METEOR scores...\")\n",
        "meteor = [\n",
        "    single_meteor_score(ref.split(), hyp.split()) * 100\n",
        "    for ref, hyp in tqdm(zip(refs, hyps), total=len(refs))\n",
        "]\n",
        "\n",
        "# 6) Add scores to the DataFrame\n",
        "df_scores_base[\"BLEU-1\"]  = bleu1\n",
        "df_scores_base[\"BLEU-2\"]  = bleu2\n",
        "df_scores_base[\"ROUGE-1\"] = rouge1\n",
        "df_scores_base[\"ROUGE-2\"] = rouge2\n",
        "df_scores_base[\"ROUGE-L\"] = rougeL\n",
        "df_scores_base[\"METEOR\"]  = meteor\n",
        "\n",
        "# 7)\n",
        "df_scores_base = df_scores_base.round(3)\n",
        "\n",
        "\n",
        "# 8) Compute and print average scores\n",
        "metrics = [\"BLEU-1\", \"BLEU-2\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"METEOR\"]\n",
        "df_scores_base = df_scores_base.round(3)\n",
        "means = df_scores_base[metrics].mean()\n",
        "\n",
        "print(\"\\n--- Average Metric Scores ---\")\n",
        "for metric, value in means.items():\n",
        "    print(f\"{metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "id": "QmxXBljv2a9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores_base.head()"
      ],
      "metadata": {
        "id": "mth4zWrT6YJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSUj-76e6ZEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}