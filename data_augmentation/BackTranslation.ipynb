{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Paths\n",
    "train_path = \"/content/drive/MyDrive/datasets/train_100k.csv\"\n",
    "test_path = \"/content/drive/MyDrive/datasets/test_1k.csv\"\n",
    "\n",
    "# Load full datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df = train_df[[\"abstract\", \"title\"]]\n",
    "test_df = test_df[[\"abstract\", \"title\"]]\n",
    "\n",
    "train_sample = train_df.sample(n=1000, random_state=42)\n",
    "test_sample = test_df.sample(n=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Load translation models\n",
    "en_to_fr_model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "fr_to_en_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "\n",
    "en_to_fr_tokenizer = MarianTokenizer.from_pretrained(en_to_fr_model_name)\n",
    "en_to_fr_model = MarianMTModel.from_pretrained(en_to_fr_model_name).to(\"cuda\")\n",
    "\n",
    "fr_to_en_tokenizer = MarianTokenizer.from_pretrained(fr_to_en_model_name)\n",
    "fr_to_en_model = MarianMTModel.from_pretrained(fr_to_en_model_name).to(\"cuda\")\n",
    "\n",
    "def back_translate(text, batch_size=1):\n",
    "    # English → French\n",
    "    inputs = en_to_fr_tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    translated = en_to_fr_model.generate(**inputs, max_new_tokens=256)\n",
    "    fr_text = en_to_fr_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # French → English\n",
    "    inputs = fr_to_en_tokenizer([fr_text], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    back_translated = fr_to_en_model.generate(**inputs, max_new_tokens=256)\n",
    "    en_text = fr_to_en_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    return en_text\n",
    "\n",
    "# 2. Back-translate abstracts (start with e.g. 1000 samples)\n",
    "augmented_rows = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Back-translating\"):\n",
    "    try:\n",
    "        new_abstract = back_translate(row[\"abstract\"])\n",
    "        augmented_rows.append({\n",
    "            \"abstract\": new_abstract,\n",
    "            \"title\": row[\"title\"]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping index {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 3. Convert the augmented data into a DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "\n",
    "# 4. Combine the original and augmented data\n",
    "final_train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "\n",
    "# 5. Save the new dataframe (optional)\n",
    "final_train_df.to_csv(\"/content/augmented_train.csv\", index=False)\n",
    "\n",
    "print(\"Augmented dataset has\", len(final_train_df), \"rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_v_1_13_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
